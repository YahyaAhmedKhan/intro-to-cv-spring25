1. torch.rand(3, 4) + torch.rand(4, 3) 
   - Error: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1.

2. torch.randn(3, 4) + torch.randn(4,) 
   - Output: tensor([[ 1.0489,  2.0216, -0.9239,  1.3273], [ 1.6904,  1.6627, -0.2893,  1.3192], [ 1.2586,  1.9307,  1.1280,  0.8915]])
   - Unexpected: I thought it would give an error, as they are different shapes.

3. torch.randn(3, 4) + torch.randn(4, 1) 
   - Error: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0.

4. torch.tensor([1, 0], dtype=torch.float32) + torch.tensor([2, 1], dtype=torch.float64) 
   - Output: tensor([3., 1.], dtype=torch.float64)
   - Expected: It cast the float32 to float64.

5. t = torch.zeros(3, dtype=torch.uint8); t[0] = -3.14 
   - Error: value cannot be converted to type uint8_t without overflow.

6. torch.tensor(3, device="cpu") + torch.tensor(4, device="cuda") 
   - Output: tensor(7, device='cuda:0')
   - Unexpected: I thought it would give an error.

7. 
   x = torch.randn(10)
   y = torch.randn(10)
   plt.scatter(x, y)
   - Output: <matplotlib.collections.PathCollection at 0x7dd765ad2450>
   - Expected: Drew a plot.

8. 
   x = torch.randn(10, device="cuda")
   y = torch.randn(10, device="cuda")
   plt.scatter(x, y)
   - Error: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

9. 
   x = torch.randn(10, requires_grad=True)
   y = torch.randn(10, requires_grad=True)
   plt.scatter(x.detach(), y.detach())
   - Output: <matplotlib.collections.PathCollection at 0x7dd764048f10>
   - Expected: Drew the plot after removing the grads

10. 
    t = torch.tensor([2, 1])
    t += 1
    - Output: tensor([3, 2])
    - Expected: Added 1 after expanding along all the appropriate dimensions

11. 
    t = torch.tensor([2, 1], requires_grad=True)
    t += 1
    - Error: a leaf Variable that requires grad is being used in an in-place operation.

12. 
    x = torch.tensor(100.0, requires_grad=True)
    y = x**2
    dy_dx = torch.autograd.grad(y, x)[0]
    - Output: The derivative of y = x^2 at x = 100 is: 200.0
    - Expected: Correct differential found

13. 
    x = torch.tensor([100.0, 100.0], requires_grad=True)
    y = x**2
    dy_dx = torch.autograd.grad(y, x)[0]
    - Error: grad can be implicitly created only for scalar outputs.

14. 
    x = torch.tensor(100.0, requires_grad=True)
    y = torch.log(torch.exp(x))
    dy_dx = torch.autograd.grad(y, x)[0]
    - Output: The derivative of y = x^2 at x = 100 is: nan
    - Unexpected: I thought it would simplay say 1, but it could not calulcate it.

15. 
    big = torch.tensor((5000, 1000, 1000, 5000), dtype=torch.float64, device='cuda')
    - Error: OutOfMemoryError: CUDA out of memory. Tried to allocate 186264.52 GiB. GPU 0 has a total capacity of 14.74 GiB 
      of which 14.62 GiB is free. Process 22478 has 118.00 MiB memory in use. Of the allocated memory 3.00 KiB is allocated 
      by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated.

16. 
    img, label = dataset[0]  # Assuming img is from CIFAR-10 dataset
    plt.imshow(img)
    - Error: Invalid shape (3, 32, 32) for image data.

17. 
    plt.imshow(img.permute(1, 2, 0))
    - Output: <matplotlib.image.AxesImage at 0x7dd7081a2510>
    - Expected: fixed format so image shows